We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes.
我们训练了一个大型深度卷积神经网络来将`ImageNet LSVRC-2010`竞赛的120万高分辨率的图像分到1000不同的类别中。
On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art.
在测试数据上，我们得到了`top-1 37.5%, top-5 17.0%`的错误率，这个结果比目前的最好结果好很多。
The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.
这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。
To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.
为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。
To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective.
为了减少全连接层的过拟合，我们采用了一个最近开发的名为`dropout`的正则化方法，结果证明是非常有效的。
We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
我们也使用这个模型的一个变种参加了`ILSVRC-2012`竞赛，赢得了冠军并且与第二名 `top-5 26.2%`的错误率相比，我们取得了`top-5 15.3%`的错误率。
Current approaches to object recognition make essential use of machine learning methods.
当前的目标识别方法基本上都使用了机器学习方法。
To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.
为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。
Until recently, datasets of labeled images were relatively small -- on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]).
直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。
Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations.
简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。
For example, the current best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].
例如，目前在MNIST数字识别任务上（<0.3%）的最好准确率已经接近了人类水平[4]。
But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images.
但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。
The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.
新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet[6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。
To learn about thousands of objects from millions of images, we need a model with a large learning capacity.
为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。
However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have.
然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。
Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26].
卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。
Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies).
它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。
Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.
因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。