We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes.
我们训练了一个大型深度卷积神经网络来将`ImageNet LSVRC-2010`竞赛的120万高分辨率的图像分到1000不同的类别中。
On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art.
在测试数据上，我们得到了`top-1 37.5%, top-5 17.0%`的错误率，这个结果比目前的最好结果好很多。
The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.
这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。
To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.
为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。
To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective.
为了减少全连接层的过拟合，我们采用了一个最近开发的名为`dropout`的正则化方法，结果证明是非常有效的。
We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
我们也使用这个模型的一个变种参加了`ILSVRC-2012`竞赛，赢得了冠军并且与第二名 `top-5 26.2%`的错误率相比，我们取得了`top-5 15.3%`的错误率。
Current approaches to object recognition make essential use of machine learning methods.
当前的目标识别方法基本上都使用了机器学习方法。
To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting.
为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。
Until recently, datasets of labeled images were relatively small -- on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]).
直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。
Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations.
简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。
For example, the current best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].
例如，目前在MNIST数字识别任务上（<0.3%）的最好准确率已经接近了人类水平[4]。
But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images.
但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。
The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.
新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet[6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。
